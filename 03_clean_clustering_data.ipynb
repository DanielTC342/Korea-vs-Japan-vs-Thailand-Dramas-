{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d0539d",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning for Clustering\n",
    "\n",
    "**Notebook:** `03_clean_clustering_data.ipynb`\n",
    "\n",
    "The current objectives of this Jupyter Notebooks are:\n",
    "\n",
    "- Select attributes relevant for clustering\n",
    "- Clean and convert numeric attributes\n",
    "- Encode categorical variables appropriately\n",
    "- Prepares a dataset suitable for unsupervised learning\n",
    "- Saves the clustering dataset for Weka\n",
    "\n",
    "# 3.1 Create the \"Clustering\" dataframe\n",
    "\n",
    "For the clustering task, the following columns are required: country, genres, tot_eps, ep_duration and score.\n",
    "\n",
    "Because k-means (the algorithm chosen for this task) is based on distance calculations, it is highly sensitive to large differences in numerical scales. Therefore, the objective of the transformations applied in this step is to ensure that all numerical features are kept within a comparable range, ideally between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae430657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drama_id', 'drama_name', 'native_name', 'year', 'synopsis', 'genres',\n",
      "       'tags', 'director', 'sc_writer', 'country', 'type', 'tot_eps',\n",
      "       'ep_duration', 'start_dt', 'end_dt', 'aired_on', 'org_net',\n",
      "       'tot_user_score', 'tot_num_user', 'tot_watched', 'content_rt', 'rank',\n",
      "       'popularity', 'dataset'],\n",
      "      dtype='object')\n",
      "clustering cols:  Index(['country', 'genres', 'tot_eps', 'ep_duration', 'tot_user_score'], dtype='object')\n",
      "clustering info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5442 entries, 0 to 5441\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         5428 non-null   object \n",
      " 1   genres          5406 non-null   object \n",
      " 2   tot_eps         5399 non-null   object \n",
      " 3   ep_duration     5262 non-null   object \n",
      " 4   tot_user_score  5251 non-null   float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 212.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the original dataset\n",
    "\n",
    "clustering_path = r\"your_dataset.csv\"\n",
    "\n",
    "clustering = pd.read_csv(clustering_path)\n",
    "\n",
    "# Keep only the needed columns\n",
    "\n",
    "print(clustering.columns)\n",
    "\n",
    "clustering_cols = ['country', 'genres', 'tot_eps', 'ep_duration', 'tot_user_score']\n",
    "\n",
    "clustering = clustering[clustering_cols]\n",
    "\n",
    "print(\"clustering cols: \", clustering.columns)\n",
    "print(\"clustering info: \")\n",
    "print(clustering.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342a3f1",
   "metadata": {},
   "source": [
    "# 3.2 Drop null values\n",
    "\n",
    "As observed in the notebook 02_clean_classification_data.ipynb, the dataset contains a small number of missing values that appear to be Missing at Random (MAR). Given the relatively low proportion of missing data and the sufficient size of the dataset, all rows containing null values are removed for the clustering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7810b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5048 entries, 0 to 5440\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         5048 non-null   object \n",
      " 1   genres          5048 non-null   object \n",
      " 2   tot_eps         5048 non-null   float64\n",
      " 3   ep_duration     5048 non-null   float64\n",
      " 4   tot_user_score  5048 non-null   float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 236.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Before dropping, clean the dtype of the numeric columns\n",
    "\n",
    "clustering['tot_eps'] = pd.to_numeric(clustering['tot_eps'], errors='coerce')\n",
    "\n",
    "clustering['ep_duration'] = pd.to_numeric(clustering['ep_duration'], errors='coerce')\n",
    "\n",
    "# Keep only the rows where the country are actually the needed countries\n",
    "\n",
    "clustering['country'] = clustering['country'].astype(str).str.strip()\n",
    "\n",
    "clustering = clustering[(clustering['country'] == 'Japan') | (clustering['country'] == 'South Korea')| (clustering['country'] == 'Thailand')]\n",
    "\n",
    "# Drop na's\n",
    "\n",
    "clustering.dropna(subset=clustering_cols, inplace=True)\n",
    "\n",
    "print(\"clustering info:\")\n",
    "print(clustering.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad774d5",
   "metadata": {},
   "source": [
    "In total, 394 rows (approximately 7% of the dataset) were dropped. The remaining data is still sufficient to perform the clustering analysis without affecting the validity of the results.\n",
    "\n",
    "# 3.3 Transform numerical data\n",
    "\n",
    "## 3.3.1 Clean the tot_eps and ep_duration data\n",
    "\n",
    "Filter those 0 values that were correctly transformed into float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "95009632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5048 entries, 0 to 5440\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         5048 non-null   object \n",
      " 1   genres          5048 non-null   object \n",
      " 2   tot_eps         5048 non-null   float64\n",
      " 3   ep_duration     5048 non-null   float64\n",
      " 4   tot_user_score  5048 non-null   float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 236.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# tot_eps\n",
    "\n",
    "clustering = clustering[(clustering['tot_eps'] != 0.0)]\n",
    "\n",
    "# ep_duration\n",
    "\n",
    "clustering = clustering[(clustering['ep_duration'] != 0.0)]\n",
    "\n",
    "# Valite there are not null values\n",
    "\n",
    "print(clustering.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7021c",
   "metadata": {},
   "source": [
    "## 3.3.3 Transform numerical data into a 0 to 1 scale\n",
    "\n",
    "As mentioned at the beginning of this notebook, k-means is highly sensitive to differences in numerical scales. For this reason, all numerical variables must be transformed to a common scale.\n",
    "\n",
    "Although in 02_clean_classification_data.ipynb we observed that tot_eps contains many outliers and applied StandardScaler for the classification task, in this case (given how k-means computes distances), all numerical columns will be scaled using MinMaxScaler, transforming their values to a range between 0 and 1.\n",
    "\n",
    "Additionally, using a 0–1 scale allows the numerical variables to be directly comparable with the transformed categorical variables in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fe75cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tot_eps  ep_duration  tot_user_score\n",
      "0  0.024291     0.388158        0.766667\n",
      "1  0.008097     0.098684        0.777778\n",
      "2  0.036437     0.519737        0.855556\n",
      "3  0.028340     0.157895        0.788889\n",
      "4  0.044534     0.453947        0.833333\n",
      "Index(['country', 'genres', 'tot_eps', 'ep_duration', 'tot_user_score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import the needed libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Perform MinMax\n",
    "\n",
    "numercial_data = ['tot_eps', 'ep_duration', 'tot_user_score']\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "for column in numercial_data:\n",
    "    numerical_values = clustering[column]\n",
    "    numerical_values_reshaped = np.array(numerical_values).reshape(-1,1) # Reshaped into a 2d array \"[[]]\"\n",
    "    clustering[column] = minmax.fit_transform(numerical_values_reshaped)\n",
    "\n",
    "print(clustering[numercial_data].head())\n",
    "print(clustering.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f4fea",
   "metadata": {},
   "source": [
    "# 3.4 Transform categorical variables\n",
    "\n",
    "Because k-means expects numerical values on a comparable scale (0 to 1), multi-hot encoding and one-hot encoding will be applied to the genres and country columns, respectively, following the same approach used in the classification dataset.\n",
    "\n",
    "However, due to the high dimensionality generated by the genres column after multi-hot encoding, Principal Component Analysis (PCA) will be applied to the genre features in order to reduce dimensionality while preserving the most relevant variance for the clustering task.\n",
    "\n",
    "## 3.4.1 Transform country column\n",
    "\n",
    "As observed during the initial exploratory analysis (Section 2.4.1), the *country* attribute contains three distinct categories: **South Korea**, **Japan**, and **Thailand**. To convert this categorical variable into a numerical format compatible with Weka’s Logistic Regression model, **one-hot encoding** was applied using the `get_dummies` method from the *pandas* library.\n",
    "\n",
    "This transformation preserves the nominal nature of the variable while preventing the introduction of unintended ordinal relationships between countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a709de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres country 3\n",
      "clustering: \n",
      "       country                                    genres   tot_eps  \\\n",
      "0  South Korea       Thriller,  Mystery,  Comedy,  Drama  0.024291   \n",
      "1  South Korea                                    Sitcom  0.008097   \n",
      "2  South Korea  Historical,  Romance,  Drama,  Melodrama  0.036437   \n",
      "3  South Korea          Music,  Comedy,  Romance,  Youth  0.028340   \n",
      "4  South Korea  Action,  Mystery,  Comedy,  Supernatural  0.044534   \n",
      "\n",
      "   ep_duration  tot_user_score  Japan  South Korea  Thailand  \n",
      "0     0.388158        0.766667  False         True     False  \n",
      "1     0.098684        0.777778  False         True     False  \n",
      "2     0.519737        0.855556  False         True     False  \n",
      "3     0.157895        0.788889  False         True     False  \n",
      "4     0.453947        0.833333  False         True     False  \n",
      "clustering columns:  Index(['country', 'genres', 'tot_eps', 'ep_duration', 'tot_user_score',\n",
      "       'Japan', 'South Korea', 'Thailand'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create dummies\n",
    "\n",
    "print(\"genres country\", clustering['country'].nunique())\n",
    "\n",
    "countries = pd.get_dummies(clustering['country'])\n",
    "\n",
    "# Join them with original dataframe\n",
    "\n",
    "clustering = clustering.join(countries)\n",
    "\n",
    "print(\"clustering: \")\n",
    "print(clustering.head())\n",
    "print(\"clustering columns: \", clustering.columns)\n",
    "\n",
    "# Drop country column\n",
    "clustering.drop(columns=['country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e714f0",
   "metadata": {},
   "source": [
    "## 3.4.2 Transform genres column\n",
    "\n",
    "In this notebook, an exploratory data analysis (EDA) is performed on the genres column in order to understand how the data is stored in each row and to identify the true unique genre values. This step is necessary to correctly encode the column using MultiLabelBinarizer, following the same approach used in 02_clean_classification_data.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "46a0602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres unique 1090\n",
      "clustering data:\n",
      "0         Thriller,  Mystery,  Comedy,  Drama\n",
      "1                                      Sitcom\n",
      "2    Historical,  Romance,  Drama,  Melodrama\n",
      "3            Music,  Comedy,  Romance,  Youth\n",
      "4    Action,  Mystery,  Comedy,  Supernatural\n",
      "Name: genres, dtype: object\n",
      "clustering['genres'] split:  0         [Thriller,   Mystery,   Comedy,   Drama]\n",
      "1                                         [Sitcom]\n",
      "2    [Historical,   Romance,   Drama,   Melodrama]\n",
      "3            [Music,   Comedy,   Romance,   Youth]\n",
      "4    [Action,   Mystery,   Comedy,   Supernatural]\n",
      "Name: genres, dtype: object\n",
      "Unique genres: ['Thriller' 'Mystery' 'Comedy' 'Drama' 'Sitcom' 'Historical' 'Romance'\n",
      " 'Melodrama' 'Music' 'Youth' 'Action' 'Supernatural' 'Military' 'Fantasy'\n",
      " 'Horror' 'Psychological' 'Life' 'Crime' 'Food' 'Adventure' 'Sci-Fi'\n",
      " 'Business' 'Medical' 'Family' 'Political' 'Law' 'Mature' 'Sports'\n",
      " 'Documentary' 'Martial Arts' 'Tokusatsu' 'War' 'Wuxia']\n",
      "genre_df:\n",
      "   Action  Adventure  Business  Comedy  Crime  Documentary  Drama  Family  \\\n",
      "0       0          0         0       1      0            0      1       0   \n",
      "1       0          0         0       0      0            0      0       0   \n",
      "2       0          0         0       0      0            0      1       0   \n",
      "3       0          0         0       1      0            0      0       0   \n",
      "4       1          0         0       1      0            0      0       0   \n",
      "\n",
      "   Fantasy  Food  ...  Romance  Sci-Fi  Sitcom  Sports  Supernatural  \\\n",
      "0        0     0  ...        0       0       0       0             0   \n",
      "1        0     0  ...        0       0       1       0             0   \n",
      "2        0     0  ...        1       0       0       0             0   \n",
      "3        0     0  ...        1       0       0       0             0   \n",
      "4        0     0  ...        0       0       0       0             1   \n",
      "\n",
      "   Thriller  Tokusatsu  War  Wuxia  Youth  \n",
      "0         1          0    0      0      0  \n",
      "1         0          0    0      0      0  \n",
      "2         0          0    0      0      0  \n",
      "3         0          0    0      0      1  \n",
      "4         0          0    0      0      0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "genre_df:  Index(['Action', 'Adventure', 'Business', 'Comedy', 'Crime', 'Documentary',\n",
      "       'Drama', 'Family', 'Fantasy', 'Food', 'Historical', 'Horror', 'Law',\n",
      "       'Life', 'Martial Arts', 'Mature', 'Medical', 'Melodrama', 'Military',\n",
      "       'Music', 'Mystery', 'Political', 'Psychological', 'Romance', 'Sci-Fi',\n",
      "       'Sitcom', 'Sports', 'Supernatural', 'Thriller', 'Tokusatsu', 'War',\n",
      "       'Wuxia', 'Youth'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Perform EDA on the columns\n",
    "\n",
    "print(\"genres unique\", clustering['genres'].nunique())\n",
    "\n",
    "print(\"clustering data:\")\n",
    "print(clustering['genres'].head()) # Base on this we prove that we have \n",
    "\n",
    "# Split the data\n",
    "\n",
    "clustering['genres'] = clustering['genres'].fillna('').str.split(',')\n",
    "\n",
    "print(\"clustering['genres'] split: \", clustering['genres'].head())\n",
    "\n",
    "# Use the method explode to break each list into single data\n",
    "\n",
    "clustering_exploded = clustering.explode('genres')\n",
    "clustering_exploded['genres'] = clustering_exploded['genres'].str.strip()\n",
    "\n",
    "print('Unique genres:', clustering_exploded['genres'].unique())\n",
    "\n",
    "# Import the needed values\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb=MultiLabelBinarizer()\n",
    "\n",
    "# Clean the genres data\n",
    "clustering['genres'] = clustering['genres'].apply(lambda row_list: [str.strip(item) for item in row_list])\n",
    "\n",
    "# Create the matrix to tranform the genders into numeric values\n",
    "genre_matrix = mlb.fit_transform(clustering['genres'])\n",
    "\n",
    "# Create a dataframe that contains the columns of the matrix\n",
    "genre_df = pd.DataFrame(genre_matrix, columns=mlb.classes_, index=clustering.index)\n",
    "print(\"genre_df:\")\n",
    "print(genre_df.head())\n",
    "print(\"genre_df: \", genre_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fc39e",
   "metadata": {},
   "source": [
    "Once the data has been transformed using MultiLabelBinarizer, a Principal Component Analysis (PCA) is applied to reduce dimensionality and improve clustering results. This decision is made because a high number of features can introduce noise and negatively affect k-means, which relies on distance calculations. Reducing dimensionality helps preserve the most relevant information while improving cluster quality and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6491956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PC1': 'Romance-Comedy', 'PC2': 'Drama-Romance', 'PC3': 'Romance-Mystery', 'PC4': 'Life-Youth', 'PC5': 'Youth-Drama', 'PC6': 'Mystery-Life', 'PC7': 'Action-Thriller', 'PC8': 'Supernatural-Fantasy', 'PC9': 'Fantasy-Mystery', 'PC10': 'Thriller-Fantasy', 'PC11': 'Melodrama-Family'}\n",
      "clustering df:\n",
      "    tot_eps  ep_duration  tot_user_score  Japan  South Korea  Thailand  \\\n",
      "0  0.024291     0.388158        0.766667  False         True     False   \n",
      "1  0.008097     0.098684        0.777778  False         True     False   \n",
      "2  0.036437     0.519737        0.855556  False         True     False   \n",
      "3  0.028340     0.157895        0.788889  False         True     False   \n",
      "4  0.044534     0.453947        0.833333  False         True     False   \n",
      "\n",
      "   Romance-Comedy  Drama-Romance  Romance-Mystery  Life-Youth  Youth-Drama  \\\n",
      "0       -0.738250      -0.160334        -0.509042   -1.027447     0.324786   \n",
      "1       -0.446158      -0.363106        -0.024570    0.342908    -0.119056   \n",
      "2        0.248434       0.796581         0.334224   -0.091373    -0.301989   \n",
      "3        0.813825      -0.627013         0.111923    0.172373     0.790500   \n",
      "4       -0.552039      -1.062432        -0.065306   -0.621414     0.054555   \n",
      "\n",
      "   Mystery-Life  Action-Thriller  Supernatural-Fantasy  Fantasy-Mystery  \\\n",
      "0      0.587373         0.168049             -0.119354        -0.010507   \n",
      "1     -0.422471        -0.271270             -0.226123        -0.062079   \n",
      "2     -0.202128        -0.237315             -0.166328         0.119576   \n",
      "3      0.043432         0.012654             -0.110864        -0.027819   \n",
      "4     -0.061355         0.273239              0.914728        -0.308550   \n",
      "\n",
      "   Thriller-Fantasy  Melodrama-Family  \n",
      "0          0.322989          0.011242  \n",
      "1         -0.048611         -0.098909  \n",
      "2         -0.053211          0.573645  \n",
      "3          0.000614         -0.012315  \n",
      "4         -0.538681          0.340337  \n",
      "clustering cols:  Index(['tot_eps', 'ep_duration', 'tot_user_score', 'Japan', 'South Korea',\n",
      "       'Thailand', 'Romance-Comedy', 'Drama-Romance', 'Romance-Mystery',\n",
      "       'Life-Youth', 'Youth-Drama', 'Mystery-Life', 'Action-Thriller',\n",
      "       'Supernatural-Fantasy', 'Fantasy-Mystery', 'Thriller-Fantasy',\n",
      "       'Melodrama-Family'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "\n",
    "pca = PCA(n_components=11) # Keep a 33% of the total components\n",
    "\n",
    "genre_pca = pca.fit_transform(genre_df)\n",
    "\n",
    "# To get a more explicit headers extract the two principal genders of hour 11 components\n",
    "\n",
    "components_dic = {}\n",
    "\n",
    "for i in range(pca.n_components_):\n",
    "    # Get loadings for component i\n",
    "    comp_recipe = pd.DataFrame(\n",
    "        pca.components_[i],\n",
    "        index=mlb.classes_,\n",
    "        columns=['Weight']\n",
    "    )\n",
    "    \n",
    "    # Top 2 genres (positive correlation)\n",
    "    top_genres = comp_recipe.sort_values(by='Weight', ascending=False).head(2).index.tolist()\n",
    "    \n",
    "    # Optional: also look at bottom 2 (negative correlation)\n",
    "    bottom_genres = comp_recipe.sort_values(by='Weight', ascending=True).head(2).index.tolist()\n",
    "    \n",
    "    # Create a descriptive name (e.g., \"Romance-Drama\")\n",
    "    comp_name = \"-\".join(top_genres)\n",
    "    \n",
    "    # Save in dictionary\n",
    "    components_dic[f'PC{i+1}'] = comp_name\n",
    "\n",
    "print(components_dic)\n",
    "\n",
    "pca_columns = [f'PC{i+1}' for i in range(genre_pca.shape[1])]\n",
    "genre_pca_df = pd.DataFrame(genre_pca, columns=pca_columns, index=clustering.index)\n",
    "\n",
    "\n",
    "# Join PCA features back to original dataframe\n",
    "clustering = clustering.join(genre_pca_df)\n",
    "\n",
    "clustering.rename(columns=components_dic, inplace=True)\n",
    "\n",
    "# Drop original genres column (since you now have PCA features)\n",
    "clustering.drop(columns=['genres'], inplace=True)\n",
    "\n",
    "print(\"clustering df:\")\n",
    "print(clustering.head())\n",
    "print(\"clustering cols: \", clustering.columns)\n",
    "\n",
    "# Clean the countries dtype\n",
    "\n",
    "countries_list = ['Japan', 'South Korea', 'Thailand']\n",
    "\n",
    "for country in countries_list:\n",
    "    clustering[country] = pd.to_numeric(clustering[country], errors='coerce').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d43ff6",
   "metadata": {},
   "source": [
    "# 3.5 Save the dataframe to use it with Weka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1d62401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering cols:  Index(['tot_eps', 'ep_duration', 'tot_user_score', 'Japan', 'South Korea',\n",
      "       'Thailand', 'Romance-Comedy', 'Drama-Romance', 'Romance-Mystery',\n",
      "       'Life-Youth', 'Youth-Drama', 'Mystery-Life', 'Action-Thriller',\n",
      "       'Supernatural-Fantasy', 'Fantasy-Mystery', 'Thriller-Fantasy',\n",
      "       'Melodrama-Family'],\n",
      "      dtype='object')\n",
      "clustering:\n",
      "    tot_eps  ep_duration  tot_user_score  Japan  South Korea  Thailand  \\\n",
      "0  0.024291     0.388158        0.766667      0            1         0   \n",
      "1  0.008097     0.098684        0.777778      0            1         0   \n",
      "2  0.036437     0.519737        0.855556      0            1         0   \n",
      "3  0.028340     0.157895        0.788889      0            1         0   \n",
      "4  0.044534     0.453947        0.833333      0            1         0   \n",
      "\n",
      "   Romance-Comedy  Drama-Romance  Romance-Mystery  Life-Youth  Youth-Drama  \\\n",
      "0       -0.738250      -0.160334        -0.509042   -1.027447     0.324786   \n",
      "1       -0.446158      -0.363106        -0.024570    0.342908    -0.119056   \n",
      "2        0.248434       0.796581         0.334224   -0.091373    -0.301989   \n",
      "3        0.813825      -0.627013         0.111923    0.172373     0.790500   \n",
      "4       -0.552039      -1.062432        -0.065306   -0.621414     0.054555   \n",
      "\n",
      "   Mystery-Life  Action-Thriller  Supernatural-Fantasy  Fantasy-Mystery  \\\n",
      "0      0.587373         0.168049             -0.119354        -0.010507   \n",
      "1     -0.422471        -0.271270             -0.226123        -0.062079   \n",
      "2     -0.202128        -0.237315             -0.166328         0.119576   \n",
      "3      0.043432         0.012654             -0.110864        -0.027819   \n",
      "4     -0.061355         0.273239              0.914728        -0.308550   \n",
      "\n",
      "   Thriller-Fantasy  Melodrama-Family  \n",
      "0          0.322989          0.011242  \n",
      "1         -0.048611         -0.098909  \n",
      "2         -0.053211          0.573645  \n",
      "3          0.000614         -0.012315  \n",
      "4         -0.538681          0.340337  \n",
      "clustering info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5048 entries, 0 to 5440\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   tot_eps               5048 non-null   float64\n",
      " 1   ep_duration           5048 non-null   float64\n",
      " 2   tot_user_score        5048 non-null   float64\n",
      " 3   Japan                 5048 non-null   int64  \n",
      " 4   South Korea           5048 non-null   int64  \n",
      " 5   Thailand              5048 non-null   int64  \n",
      " 6   Romance-Comedy        5048 non-null   float64\n",
      " 7   Drama-Romance         5048 non-null   float64\n",
      " 8   Romance-Mystery       5048 non-null   float64\n",
      " 9   Life-Youth            5048 non-null   float64\n",
      " 10  Youth-Drama           5048 non-null   float64\n",
      " 11  Mystery-Life          5048 non-null   float64\n",
      " 12  Action-Thriller       5048 non-null   float64\n",
      " 13  Supernatural-Fantasy  5048 non-null   float64\n",
      " 14  Fantasy-Mystery       5048 non-null   float64\n",
      " 15  Thriller-Fantasy      5048 non-null   float64\n",
      " 16  Melodrama-Family      5048 non-null   float64\n",
      "dtypes: float64(14), int64(3)\n",
      "memory usage: 709.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"clustering cols: \", clustering.columns)\n",
    "\n",
    "print(\"clustering:\")\n",
    "print(clustering.head())\n",
    "\n",
    "print(\"clustering info:\")\n",
    "print(clustering.info())\n",
    "\n",
    "clustering.to_csv(\"clustering.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
